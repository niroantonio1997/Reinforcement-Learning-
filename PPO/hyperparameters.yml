flappybird:
  env_id: FlappyBird-v0
  gamma: 0.99
  buffer_size: 2048
  train_freq: 10
  max_episode_steps: 1000
  max_episodes: 100000
  total_timesteps: 500000
  max_reward: 100
  actor_lr:  1e-4
  critic_lr: 5e-4
  actor_hidden_dims: [1024]
  critic_hidden_dims: [1024]
  epsilon: 0.2
  entropy_coef: 0.1
  entropy_coef_decay: 0.99  # Decay factor for entropy coefficient
  action_type: discrete

cartpole:
  env_id: CartPole-v1
  gamma: 0.99
  buffer_size: 4096  # Buffer più grande per più stabilità
  train_freq: 3  # Meno epoche per evitare overfitting
  max_episode_steps: 500
  max_episodes: 15000  # Più episodi per dare tempo di imparare
  total_timesteps: 50000
  max_reward: 475  # CartPole is solved at 475 average reward
  action_type: discrete
  actor_hidden_dims: [32, 32]  # Rete più piccola per evitare overfitting
  critic_hidden_dims: [32, 32]
  actor_lr: 5e-4  # Learning rate intermedio
  critic_lr: 5e-4  # Stesso learning rate per stabilità
  entropy_coef: 0.02  # Più exploration
  entropy_coef_decay: 0.998  # Decay più lento
  epsilon: 0.2
  minibatch_size: 128  # Batch più grandi

lunarlander:
  env_id: LunarLander-v3
  gamma: 0.99
  buffer_size: 4096
  train_freq: 5
  max_episode_steps: 1000
  max_episodes: 2000
  total_timesteps: 200000
  max_reward: 300
  action_type: discrete
  actor_hidden_dims: [128, 128]
  critic_hidden_dims: [128, 128]
  actor_lr: 3e-4
  critic_lr: 1e-3
  entropy_coef: 0.01
  entropy_coef_decay: 0.998  # Decay factor for entropy coefficient

pendulum:
  env_id: Pendulum-v1
  gamma: 0.99
  buffer_size: 4096        # Buffer più grande per stabilità
  train_freq: 8            # Più epoche per ambiente che non termina
  max_episode_steps: 200
  max_episodes: 2000       # Più episodi per convergenza
  total_timesteps: 400000  # Più timesteps totali
  max_reward: 0            # Reward massimo è 0 (migliore performance)
  action_type: continuous
  actor_hidden_dims: [128, 128]     # Reti più grandi per complessità
  critic_hidden_dims: [128, 128]
  actor_lr: 1e-3           # Learning rate più alto per Pendulum
  critic_lr: 1e-3          # Stesso learning rate
  entropy_coef: 0.05       # Entropy per esplorazione (importante!)
  entropy_coef_decay: 0.995 # Decay graduale
  std: 0.4                 # Std più alta per esplorazione iniziale
  std_decay: 0.998         # Decay molto lento
  epsilon: 0.2
  minibatch_size: 128      # Batch più grandi per stabilità
  lambda: 0.95

mountaincar:
  env_id: MountainCarContinuous-v0
  gamma: 0.99
  buffer_size: 2048
  train_freq: 5
  max_episode_steps: 999
  max_episodes: 1500
  total_timesteps: 100000
  max_reward: 90
  action_type: continuous
  actor_hidden_dims: [256, 256]
  critic_hidden_dims: [256, 256]
  actor_lr: 3e-4
  critic_lr: 1e-3
  entropy_coef: 0.1
  entropy_coef_decay: 0.99
  std: 0.4  # Aumentato per migliore esplorazione
  std_decay: 0.995  # Decay più lento
  epsilon: 0.2
  minibatch_size: 64
  lambda: 0.95

# Ambienti con azioni continue avanzati
bipedalwalker:
  env_id: BipedalWalker-v3
  gamma: 0.99
  buffer_size: 4096  # Buffer grande per azioni continue complesse
  train_freq: 10     # Più epoche per ambiente difficile
  max_episode_steps: 1600
  max_episodes: 3000
  total_timesteps: 2000000
  max_reward: 300
  action_type: continuous
  actor_hidden_dims: [256, 256]  # Reti profonde per complessità
  critic_hidden_dims: [256, 256]
  actor_lr: 3e-4
  critic_lr: 3e-4
  entropy_coef: 0.01    # Bassa entropy per policy più deterministica
  entropy_coef_decay: 0.995
  std: 0.1              # Std bassa per controllo preciso
  std_decay: 0.9995     # Decay lento
  epsilon: 0.2
  minibatch_size: 128
  lambda: 0.95

humanoid:
  env_id: Humanoid-v5
  gamma: 0.99
  buffer_size: 16000    # Buffer molto grande per alta dimensionalità
  train_freq: 50       # Molte epoche per ambiente ultra-complesso
  max_episode_steps: 2000
  max_episodes: 1000000
  total_timesteps: 5000000
  max_reward: 6000
  action_type: continuous
  actor_hidden_dims: [1025, 512, 256]  # Rete più profonda
  critic_hidden_dims: [1024, 512, 256]
  actor_lr: 1e-3
  critic_lr: 3e-3      # Critic impara più velocemente
  entropy_coef: 0.1  # Entropy molto bassa
  entropy_coef_decay: 0.999
  std: 0.1
  std_decay: 0.9999    # Decay molto lento
  epsilon: 0.2
  minibatch_size: 16000  # Batch grandi
  lambda: 0.95

ant:
  env_id: Ant-v4
  gamma: 0.99
  buffer_size: 4096
  train_freq: 8
  max_episode_steps: 1000
  max_episodes: 2500
  total_timesteps: 1500000
  max_reward: 6000
  action_type: continuous
  actor_hidden_dims: [256, 256]
  critic_hidden_dims: [256, 256]
  actor_lr: 3e-3
  critic_lr: 3e-3
  entropy_coef: 0.01
  entropy_coef_decay: 0.995
  std: 0.2
  std_decay: 0.999
  epsilon: 0.2
  minibatch_size: 4096
  lambda: 0.95

hopper:
  env_id: Hopper-v5
  gamma: 0.99
  buffer_size: 2048
  train_freq: 50
  max_episode_steps: 1000
  max_episodes: 8000
  total_timesteps: 800000
  max_reward: 3100
  action_type: continuous
  actor_hidden_dims: [512, 512]
  critic_hidden_dims: [512, 512]
  actor_lr: 5e-4
  critic_lr: 1e-3
  entropy_coef: 0.5   # Entropy più alta per esplorazione
  entropy_coef_decay: 0.99995
  std: 0.3
  std_decay: 0.998
  epsilon: 0.2
  minibatch_size: 2048
  lambda: 0.95
  max_reward: 3000  # Aggiornato per il nuovo ambiente Hopper-v5

walker2d:
  env_id: Walker2d-v4
  gamma: 0.99
  buffer_size: 2048
  train_freq: 6
  max_episode_steps: 1000
  max_episodes: 2000
  total_timesteps: 800000
  max_reward: 5000
  action_type: continuous
  actor_hidden_dims: [128, 128]
  critic_hidden_dims: [128, 128]
  actor_lr: 3e-4
  critic_lr: 3e-4
  entropy_coef: 0.01
  entropy_coef_decay: 0.995
  std: 0.1
  std_decay: 0.998
  epsilon: 0.2
  minibatch_size: 64
  lambda: 0.95

halfcheetah:
  env_id: HalfCheetah-v4
  gamma: 0.99
  buffer_size: 4096
  train_freq: 8
  max_episode_steps: 1000
  max_episodes: 2500
  total_timesteps: 1000000
  max_reward: 12000
  action_type: continuous
  actor_hidden_dims: [256, 256]
  critic_hidden_dims: [256, 256]
  actor_lr: 3e-4
  critic_lr: 1e-3
  entropy_coef: 0.005  # Bassa entropy per velocità
  entropy_coef_decay: 0.999
  std: 0.1
  std_decay: 0.9995
  epsilon: 0.2
  minibatch_size: 128
  lambda: 0.95

# Ambienti con spazio di controllo più semplice
invertedpendulum:
  env_id: InvertedPendulum-v4
  gamma: 0.99
  buffer_size: 1024
  train_freq: 4
  max_episode_steps: 1000
  max_episodes: 500
  total_timesteps: 50000
  max_reward: 1000
  action_type: continuous
  actor_hidden_dims: [64, 64]
  critic_hidden_dims: [64, 64]
  actor_lr: 1e-3      # Learning rate più alto per ambiente semplice
  critic_lr: 1e-3
  entropy_coef: 0.05  # Entropy alta per esplorazione
  entropy_coef_decay: 0.99
  std: 0.3
  std_decay: 0.995
  epsilon: 0.2
  minibatch_size: 32
  lambda: 0.95

inverteddoublependulum:
  env_id: InvertedDoublePendulum-v4
  gamma: 0.99
  buffer_size: 2048
  train_freq: 6
  max_episode_steps: 1000
  max_episodes: 10000
  total_timesteps: 200000
  max_average_reward: 6000
  action_type: continuous
  actor_hidden_dims: [128, 128]
  critic_hidden_dims: [128, 128]
  actor_lr: 5e-4
  critic_lr: 5e-4
  entropy_coef: 0.02
  entropy_coef_decay: 0.995
  std: 0.2
  std_decay: 0.995
  epsilon: 0.2
  minibatch_size: 64
  lambda: 0.95